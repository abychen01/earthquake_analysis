{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b7b361f-cd61-46e1-94ec-a889c2ef6b93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import date, timedelta\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "dates = [\n",
    "    \"2025-01-01\", \"2025-02-01\", \"2025-03-01\", \"2025-04-01\", \"2025-04-15\"\n",
    "]\n",
    "\n",
    "for i in range(4):\n",
    "\n",
    "    j= i + 1\n",
    "    url = f\"https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={dates[i]}&endtime={dates[j]}\"\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        data = response.json().get(\"features\",[])\n",
    "\n",
    "        if not data:\n",
    "            print(\"No data received\")\n",
    "        else:\n",
    "            file_path = f\"abfss://bronze@abychen.dfs.core.windows.net/historic_earthquake_data_{dates[i]}_to_{dates[j]}.json\"\n",
    "            json_data = json.dumps(data, indent=4)\n",
    "\n",
    "            dbutils.fs.put(file_path, json_data, overwrite=True)\n",
    "            #df = spark.createDataFrame([json_data])\n",
    "            print(f\"Data saved to {file_path}\")\n",
    "            #display(df)\n",
    "            #print(json_data)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Request failed: {e}\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8682c9c7-f720-45ed-8817-3c83b6ff65b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, to_timestamp, to_date, date_format, isnull\n",
    "from pyspark.sql.types import TimestampType\n",
    "i = 0\n",
    "silver_file_path = \"abfss://silver@abychen.dfs.core.windows.net/earthquake_events_silver\"\n",
    "\n",
    "for i in range(4):\n",
    "    \n",
    "    j= i + 1\n",
    "    file_path = f\"abfss://bronze@abychen.dfs.core.windows.net/historic_earthquake_data_{dates[i]}_to_{dates[j]}.json\"\n",
    "    df = spark.read.option(\"multiline\", \"true\").json(file_path)\n",
    "    \n",
    "    df2 = df.select(\n",
    "        df.id,\n",
    "        df.geometry.coordinates[0].alias(\"longitude\"),\n",
    "        df.geometry.coordinates[1].alias(\"latitude\"),\n",
    "        df.geometry.coordinates[2].alias(\"elevation\"),\n",
    "        df.properties.title.alias(\"title\"),\n",
    "        df.properties.mag.alias(\"magnitude\"),\n",
    "        df.properties.place.alias(\"place_description\"),\n",
    "        df.properties.sig.alias(\"sig\"),\n",
    "        df.properties.magType.alias(\"magType\"),\n",
    "        df.properties.time.alias(\"time\"),\n",
    "        df.properties.updated.alias(\"updated\")\n",
    "    )\n",
    "    df2 = df2.withColumn(\"longitude\", when(isnull(col(\"longitude\")), 0).otherwise(col(\"longitude\")))\\\n",
    "                    .withColumn(\"latitude\",when(isnull(col('latitude')),0).otherwise(col(\"latitude\")))\\\n",
    "                        .withColumn(\"time\",when(df2.time.isNull(),0).otherwise(df2.time))\n",
    "\n",
    "    df2 = df2.withColumn(\"time\",((df2.time)/1000).cast(TimestampType()))\\\n",
    "        .withColumn(\"updated\",((df2.updated)/1000).cast(TimestampType()))\n",
    "\n",
    "    df2 = df2.withColumn(\"event_date\", to_date(to_timestamp(col(\"time\"))))\\\n",
    "            .withColumn(\"event_time\", date_format(to_timestamp(col(\"time\")),\"HH:mm:ss:SSS\"))\\\n",
    "            .withColumn(\"updated_date\", to_date(to_timestamp(col(\"updated\"))))\\\n",
    "            .withColumn(\"updated_time\", date_format(to_timestamp(col(\"updated\")),\"HH:mm:ss:SSS\"))\n",
    "\n",
    "    df2 = df2.drop(\"time\",\"updated\")\n",
    "    df2.write.mode(\"append\").format(\"delta\").save(silver_file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e734ec4a-c409-4822-9787-62057639f8f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, udf\n",
    "import reverse_geocoder as rg\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "silver_file_path = \"abfss://silver@abychen.dfs.core.windows.net/earthquake_events_silver\"\n",
    "\n",
    "\n",
    "def get_country_code(lat,long):\n",
    "    try:\n",
    "        coordinates = (float(lat), float(long))\n",
    "        result = rg.search(coordinates)[0].get('cc')\n",
    "        print(f\"Processed coordinates: {coordinates} -> {result}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing coordinates: {lat}, {long} -> {str(e)}\")\n",
    "        return None\n",
    "\n",
    "get_country_code_udf = udf(get_country_code, StringType())\n",
    "\n",
    "df3 = spark.read.format(\"delta\").load(silver_file_path)\n",
    "\n",
    "df3 = df3.withColumn(\"Country\",get_country_code_udf(col(\"latitude\"),col(\"longitude\")))\n",
    "\n",
    "gold_adls = \"abfss://gold@abychen.dfs.core.windows.net/\"\n",
    "df3.write.format(\"delta\").mode(\"append\").save(f\"{gold_adls}EarthquakeData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11097b10-bc4f-4ca3-96cf-466cd45a659d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df3.sort(df3.event_date.desc()).limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2afb1ca-a660-483c-bda7-a3e1079b6abd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gold_adls = \"abfss://gold@abychen.dfs.core.windows.net/\"\n",
    "df3.write.format(\"delta\").mode(\"append\").save(f\"{gold_adls}EarthquakeData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2d20873-1c6f-47fb-82c8-458de57621f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "historic_data nb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
