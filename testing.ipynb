{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0a3e227-2a2f-4676-b333-07278463f16b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TESTING........\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "#from pyspark.sql.types import DateType\n",
    "from datetime import date, timedelta\n",
    "from pyspark.sql.functions import when, col, udf\n",
    "import reverse_geocoder as rg\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n",
    "start_date = date.today() - timedelta(1)\n",
    "file_path = \"abfss://gold@abychen.dfs.core.windows.net/EarthquakeData\"\n",
    "file_path2 = \"abfss://silver@abychen.dfs.core.windows.net/earthquake_events_silver\"\n",
    "\n",
    "\n",
    "df = spark.read.format(\"delta\").load(file_path).filter(col(\"event_date\")>\"2025-04-14\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4de31de9-e1bd-4ab9-869b-a03a798df40d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").mode(\"append\").save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78d260f0-6024-4994-ba42-dfd0de7ea514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start_date = date.today() - timedelta(2)\n",
    "print(start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52dfb95f-f34d-40ad-a9fc-d782088aefa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TESTING........\n",
    "\n",
    "from pyspark.sql.functions import col, current_timestamp, concat, lit, to_timestamp\n",
    "#from pyspark.sql.types import DateType\n",
    "from datetime import date, timedelta, datetime\n",
    "\n",
    "start_date = datetime.now() - timedelta(days=1)\n",
    "start_date2 = date.today()\n",
    "file_path = \"abfss://silver@abychen.dfs.core.windows.net/earthquake_events_silver\"\n",
    "\n",
    "'''\n",
    "print(start_date)\n",
    "df = spark.read.format(\"delta\").load(file_path)\n",
    "display(df)\n",
    "'''\n",
    "df = spark.read.format(\"delta\").load(file_path)\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(concat(col(\"event_date\"), lit(\" \"), col(\"event_time\")),\"yyyy-MM-dd HH:mm:ss:SSS\"))\n",
    "#display(df.filter(df.timestamp >= start_date))\n",
    "#display(df.filter(df.timestamp >= \"2025-04-15\"))\n",
    "\n",
    "print(start_date2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e54904f-47ee-45f4-915f-c5ff49b0503f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from pyspark.sql.functions import concat, lit, current_date, to_timestamp\n",
    "from datetime import date, timedelta, datetime\n",
    "\n",
    "start_date = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(1)\n",
    "end_date = datetime.now().replace(hour=23, minute=59, second=59, microsecond=0) - timedelta(1)\n",
    "\n",
    "print(start_date)\n",
    "print(end_date)\n",
    "\n",
    "url = f\"https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={start_date}&endtime={end_date}\"\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    # getting earthquake data for yesterday using API\n",
    "    response.raise_for_status()\n",
    "    # checking if response is successful\n",
    "\n",
    "    data = response.json().get(\"features\",[])\n",
    "    # getting the features from the response in GeoJSON format\n",
    "\n",
    "    if not data:\n",
    "        print(\"No data received\")\n",
    "    else:\n",
    "        #file_path = f\"{bronze_adls}{start_date}_earthquake_data.json\"\n",
    "        json_data = json.dumps(data, indent=4)\n",
    "        # converting geojson data to json\n",
    "        print(json_data)\n",
    "        #dbutils.fs.put(file_path, json_data, overwrite=True)\n",
    "        #writing to adls\n",
    "        #print(f\"Data saved to {file_path}\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Request failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26d9c8ad-1b13-413b-ae66-29dfb82b4e42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "testing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
