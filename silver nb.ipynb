{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de2778cf-ad5f-4a6a-9017-c39d1d2531dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Silver Notebook - Transform and clean the raw data stored in bronze, save in delta format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19e83230-511f-4776-8fe9-1ca0c9e5c044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get bronze output metadata (e.g., paths)\n",
    "\n",
    "bronze_output = dbutils.jobs.taskValues.get(taskKey=\"bronze\",key=\"bronze_output\")\n",
    "#file_path = \"abfss://silver@abychen.dfs.core.windows.net/2025-04-14_earthquake_data.json\"\n",
    "\n",
    "start_date = bronze_output.get(\"start_date\",\"\") # \"\" at the end is used for setting a default value if output is not present\n",
    "end_date = bronze_output.get(\"end_date\",\"\")\n",
    "bronze_adls = bronze_output.get(\"bronze_adls\",\"\")\n",
    "silver_adls = bronze_output.get(\"silver_adls\",\"\")\n",
    "gold_adls = bronze_output.get(\"gold_adls\",\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8e06796-afc7-43bb-b2df-9ddcce673591",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Import required Spark SQL functions\n",
    "\n",
    "from pyspark.sql.functions import when, col, to_timestamp, to_date, date_format, isnull\n",
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce10875a-1b69-4859-a742-e3804cbbd0a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Flatten nested schema and select relevant fields\n",
    "\n",
    "df = spark.read.option(\"multiline\", \"true\").json(bronze_adls)\n",
    "\n",
    "df2 = df.select(\n",
    "    df.id,\n",
    "    df.geometry.coordinates[0].alias(\"longitude\"),\n",
    "    df.geometry.coordinates[1].alias(\"latitude\"),\n",
    "    df.geometry.coordinates[2].alias(\"elevation\"),\n",
    "    df.properties.title.alias(\"title\"),\n",
    "    df.properties.mag.alias(\"magnitude\"),\n",
    "    df.properties.place.alias(\"place_description\"),\n",
    "    df.properties.sig.alias(\"sig\"),\n",
    "    df.properties.magType.alias(\"magType\"),\n",
    "    df.properties.time.alias(\"time\"),\n",
    "    df.properties.updated.alias(\"updated\")\n",
    ")\n",
    "df2 = df2.withColumn(\"longitude\", when(isnull(col(\"longitude\")), 0).otherwise(col(\"longitude\")))\\\n",
    "                   .withColumn(\"latitude\",when(isnull(col('latitude')),0).otherwise(col(\"latitude\")))\\\n",
    "                       .withColumn(\"time\",when(df2.time.isNull(),0).otherwise(df2.time))\n",
    "# Replace null values with defaults\n",
    "\n",
    "df2 = df2.withColumn(\"time\",((df2.time)/1000).cast(TimestampType()))\\\n",
    "       .withColumn(\"updated\",((df2.updated)/1000).cast(TimestampType()))\n",
    "# Convert UNIX epoch (ms) to timestamp\n",
    "\n",
    "df2 = df2.withColumn(\"event_date\", to_date(to_timestamp(col(\"time\"))))\\\n",
    "         .withColumn(\"event_time\", date_format(to_timestamp(col(\"time\")),\"HH:mm:ss:SSS\"))\\\n",
    "         .withColumn(\"updated_date\", to_date(to_timestamp(col(\"updated\"))))\\\n",
    "         .withColumn(\"updated_time\", date_format(to_timestamp(col(\"updated\")),\"HH:mm:ss:SSS\"))\n",
    "# Create formatted date/time columns\n",
    "\n",
    "df2 = df2.drop(\"time\",\"updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64ac4a02-d7b7-494b-a907-68c9f82bfaae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save processed data in Delta format to ADLS silver container\n",
    "\n",
    "silver_file_path = f\"{silver_adls}earthquake_events_silver\"\n",
    "df2.write.mode(\"append\").format(\"delta\").save(silver_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33974a74-7e32-404c-9edf-bad93117ca13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Output path for gold notebook\n",
    "\n",
    "dbutils.jobs.taskValues.set(key=\"silver_output\",value=silver_file_path) \n",
    "# sending the file_path as a single string"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver nb",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
